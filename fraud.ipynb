{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Fraud Detection using LLM and RAG"
      ],
      "metadata": {
        "id": "Imw3pngQpk8v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBUbIwFLpgjP"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain sentence-transformers faiss-cpu langchain-community langchain-core transformers chromadb"
      ],
      "metadata": {
        "id": "YFaHEcclpnd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet  langchain sentence_transformers"
      ],
      "metadata": {
        "id": "XwD326G4p-U4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Define sample data for fraud and non-fraud financial statements\n",
        "fraud_statements = [\n",
        "    \"The company reported inflated revenues by including sales that never occurred.\",\n",
        "    \"Financial records were manipulated to hide the true state of expenses.\",\n",
        "    \"The company failed to report significant liabilities on its balance sheet.\",\n",
        "    \"Revenue was recognized prematurely before the actual sales occurred.\",\n",
        "    \"The financial statement shows significant discrepancies in inventory records.\",\n",
        "    \"The company used off-balance-sheet entities to hide debt.\",\n",
        "    \"Expenses were understated by capitalizing them as assets.\",\n",
        "    \"There were unauthorized transactions recorded in the financial books.\",\n",
        "    \"Significant amounts of revenue were recognized without proper documentation.\",\n",
        "    \"The company falsified financial documents to secure a larger loan.\",\n",
        "    \"There were multiple instances of duplicate payments recorded as expenses.\",\n",
        "    \"The company reported non-existent assets to enhance its financial position.\",\n",
        "    \"Expenses were fraudulently categorized as business development costs.\",\n",
        "    \"The company manipulated financial ratios to meet loan covenants.\",\n",
        "    \"Significant related-party transactions were not disclosed.\",\n",
        "    \"The financial statement shows fabricated sales transactions.\",\n",
        "    \"There was intentional misstatement of cash flow records.\",\n",
        "    \"The company inflated the value of its assets to attract investors.\",\n",
        "    \"Revenue from future periods was reported in the current period.\",\n",
        "    \"The company engaged in channel stuffing to inflate sales figures.\"\n",
        "]\n",
        "\n",
        "non_fraud_statements = [\n",
        "    \"The company reported stable revenues consistent with historical trends.\",\n",
        "    \"Financial records accurately reflect all expenses and liabilities.\",\n",
        "    \"The balance sheet provides a true and fair view of the company’s financial position.\",\n",
        "    \"Revenue was recognized in accordance with standard accounting practices.\",\n",
        "    \"The inventory records are accurate and match physical counts.\",\n",
        "    \"The company’s debt is fully disclosed on the balance sheet.\",\n",
        "    \"All expenses are properly categorized and recorded.\",\n",
        "    \"Transactions recorded in the financial books are authorized and documented.\",\n",
        "    \"Revenue recognition is supported by proper documentation.\",\n",
        "    \"Financial documents were audited and found to be accurate.\",\n",
        "    \"Payments and expenses are recorded accurately without discrepancies.\",\n",
        "    \"The assets reported on the balance sheet are verified and exist.\",\n",
        "    \"Business development costs are properly recorded as expenses.\",\n",
        "    \"Financial ratios are calculated based on accurate data.\",\n",
        "    \"All related-party transactions are fully disclosed.\",\n",
        "    \"Sales transactions are accurately recorded in the financial statement.\",\n",
        "    \"Cash flow records are accurate and reflect actual cash movements.\",\n",
        "    \"The value of assets is fairly reported in the financial statements.\",\n",
        "    \"Revenue is reported in the correct accounting periods.\",\n",
        "    \"Sales figures are accurately reported without manipulation.\"\n",
        "]\n",
        "\n",
        "# Generate fraud and non-fraud data\n",
        "fraud_data = [{\"text\": statement, \"fraud_status\": \"fraud\"} for statement in fraud_statements]\n",
        "non_fraud_data = [{\"text\": random.choice(non_fraud_statements), \"fraud_status\": \"non-fraud\"} for _ in range(60)]\n",
        "\n",
        "# Combine data into a single dataset\n",
        "data = fraud_data + non_fraud_data\n",
        "random.shuffle(data)  # Shuffle data to mix fraud and non-fraud rows\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save to a CSV file\n",
        "df.to_csv(\"financial_statements_fraud_dataset.csv\", index=False)"
      ],
      "metadata": {
        "id": "bMKNKSC7qilQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "AVdePtQ0qkep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Ensure NLTK resources are downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Function to clean text\n",
        "def clean_text(text):\n",
        "    # Remove non-ASCII characters\n",
        "    text = text.encode('ascii', 'ignore').decode()\n",
        "\n",
        "    # Remove punctuation and numbers\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Join tokens back into text\n",
        "    cleaned_text = ' '.join(tokens)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "# Clean 'Fillings' column\n",
        "df['Clean_Text'] = df['text'].apply(clean_text)\n",
        "\n",
        "# Drop original 'Text' column if no longer needed\n",
        "df.drop(columns=['text'], inplace=True)\n",
        "\n",
        "# Save cleaned data back to CSV if desired\n",
        "df.to_csv('cleaned_financial_statements.csv', index=False)\n",
        "\n",
        "# Example of how the cleaned data looks like\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "By-ad_1Xqk7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "id": "WlRvq_egqoal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "documents = []\n",
        "\n",
        "# Iterate over rows using .rows() method\n",
        "for i, row_tuple in df.iterrows():\n",
        "    document = f\"id:{i}\\Fillings: {row_tuple[1]}\\Fraud_Status: {row_tuple[0]}\"\n",
        "    documents.append(Document(page_content=document))"
      ],
      "metadata": {
        "id": "S1WZXYMLqprQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0]"
      ],
      "metadata": {
        "id": "dkdbLrjcrIDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "hg_embeddings = HuggingFaceEmbeddings()"
      ],
      "metadata": {
        "id": "p88sfBRRrJEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade chromadb"
      ],
      "metadata": {
        "id": "d8cDhuRqrLGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "persist_directory = 'docs/chroma_rag/'\n",
        "langchain_chroma = Chroma.from_documents(\n",
        "    documents=documents,\n",
        "    collection_name=\"finance_data_new\",\n",
        "    embedding=hg_embeddings,\n",
        "    persist_directory=persist_directory\n",
        ")"
      ],
      "metadata": {
        "id": "bVUMDhEzrvfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login(write_permission=True)"
      ],
      "metadata": {
        "id": "-spJSmpPrv9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda, bfloat16\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "from time import time\n",
        "#import chromadb\n",
        "#from chromadb.config import Settings\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.vectorstores import Chroma"
      ],
      "metadata": {
        "id": "ciRoZBYwrxen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "id": "32loE4D0r0ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = 'HuggingFaceH4/zephyr-7b-beta'\n",
        "\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "# set quantization configuration to load large model with less GPU memory\n",
        "# this requires the `bitsandbytes` library\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")\n",
        "\n",
        "print(device)"
      ],
      "metadata": {
        "id": "ixy9MZeVr1JT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate\n",
        "!pip install -i https://pypi.org/simple/ bitsandbytes"
      ],
      "metadata": {
        "id": "wtvBsASZr3kX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "   model_id,\n",
        "    trust_remote_code=True,\n",
        "    max_new_tokens=1024\n",
        ")\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "394nRoJar592"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the query pipeline with increased max_length\n",
        "query_pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.float16,\n",
        "    max_length=6000,  # Increase max_length\n",
        "    max_new_tokens=500,  # Control the number of new tokens generated\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "id": "VkwFIXDHsSqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "def colorize_text(text):\n",
        "    for word, color in zip([\"Reasoning\", \"Question\", \"Answer\", \"Total time\"], [\"blue\", \"red\", \"green\", \"magenta\"]):\n",
        "        text = text.replace(f\"{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n",
        "    return text"
      ],
      "metadata": {
        "id": "oNVjuwd0sTVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFacePipeline(pipeline=query_pipeline)\n",
        "\n",
        "question = \"Please explain what EU AI Act is.\"\n",
        "response = llm(prompt=question)\n",
        "\n",
        "full_response =  f\"Question: {question}\\nAnswer: {response}\"\n",
        "display(Markdown(colorize_text(full_response)))"
      ],
      "metadata": {
        "id": "wMJCm8ZqsU2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.llms import HuggingFaceHub\n",
        "from IPython.display import display, Markdown\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "# Define the prompt template\n",
        "template = \"\"\"\n",
        "You are an Fraud Detection Expert in Financial Text Data, Analyse them and Predict is the Given Statement is Fraud or not?. If you don't know the answer, just say \"Sorry, I Don't Know.\"\n",
        "Question: {question}\n",
        "Context: {context}\n",
        "Answer:\n",
        "\"\"\"\n",
        "PROMPT = PromptTemplate(input_variables=[\"context\", \"query\"], template=template)\n",
        "\n",
        "# Ensure llm and langchain_chroma are properly initialized\n",
        "retriever = langchain_chroma.as_retriever(search_kwargs={\"k\": 1})\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm, retriever=retriever, chain_type_kwargs={\"prompt\": PROMPT}\n",
        ")\n",
        "\n",
        "# Define your question\n",
        "# question = \"The company reported inflated revenues by including sales that never occurred.\"\n",
        "question = \"Financial records accurately reflect all expenses and liabilities.\"\n",
        "# question = \"Revenue was recognized prematurely before the actual sales occurred.\"\n",
        "# question = \"The balance sheet provides a true and fair view of the company’s financial position.\"\n",
        "\n",
        "# Run the QA chain\n",
        "try:\n",
        "    result = qa_chain({\"query\": question})\n",
        "    display(result)\n",
        "except RuntimeError as e:\n",
        "    print(f\"RuntimeError encountered: {e}\")"
      ],
      "metadata": {
        "id": "v9gViLhGs9DB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a9mNmi_5s_JH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}