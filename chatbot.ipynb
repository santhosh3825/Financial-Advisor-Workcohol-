{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Financial GenAI Chatbot\n"
      ],
      "metadata": {
        "id": "hAXLEl8Ree9A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMlJXaXzgiGs"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the GenAI Finance Chatbot"
      ],
      "metadata": {
        "id": "E7auwS-PeqDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "XYxUN5iMeoR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the Data"
      ],
      "metadata": {
        "id": "_hy8Q8rmeuWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bank = pd.read_csv(\"/kaggle/input/bankqna/BankFAQs.csv\")\n",
        "bank.head()"
      ],
      "metadata": {
        "id": "5B6Q94Z-exAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bank[\"content\"] = bank.apply(lambda row: f\"Question: {row['Question']}\\nAnswer: {row['Answer']}\", axis=1)"
      ],
      "metadata": {
        "id": "kk_uTpt_e0Ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bank.head()"
      ],
      "metadata": {
        "id": "mB8qtMpEe12H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "id": "6NKDuXw1e5Qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents[1]"
      ],
      "metadata": {
        "id": "9ZF4jZXfe9p7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community"
      ],
      "metadata": {
        "id": "geVXa2U6fAkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "mZAVR9vLfC4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading Data into Chroma DB"
      ],
      "metadata": {
        "id": "vxrVBmjMfFer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "hg_embeddings = HuggingFaceEmbeddings()"
      ],
      "metadata": {
        "id": "WRiaz2FHfE6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "id": "rlChg2LVfP2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "persist_directory = '/kaggle/working/'\n",
        "\n",
        "langchain_chroma = Chroma.from_documents(\n",
        "    documents=documents,\n",
        "    collection_name=\"chatbot_finance\",\n",
        "    embedding=hg_embeddings,\n",
        "    persist_directory=persist_directory\n",
        ")"
      ],
      "metadata": {
        "id": "oNOGnQ8UfR1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "id": "7aQdSL9dfW-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the Zypher LLM"
      ],
      "metadata": {
        "id": "M9nSIcgkfZmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda, bfloat16\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "from time import time\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "model_id = 'HuggingFaceH4/zephyr-7b-beta'\n",
        "\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "# set quantization configuration to load large model with less GPU memory\n",
        "# this requires the `bitsandbytes` library\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")\n",
        "\n",
        "print(device)"
      ],
      "metadata": {
        "id": "pDBT4HHVfcI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "C4mmwQJLfeLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "   model_id,\n",
        "    trust_remote_code=True,\n",
        "    max_new_tokens=1024\n",
        ")\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "M3r9fmjAjbRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building Hugging Face Pipeline to Build LLM Function"
      ],
      "metadata": {
        "id": "l71n-32DjdEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the query pipeline with increased max_length\n",
        "query_pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.float16,\n",
        "    max_length=6000,  # Increase max_length\n",
        "    max_new_tokens=500,  # Control the number of new tokens generated\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "id": "WQB7yKzXjb9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the LLM"
      ],
      "metadata": {
        "id": "aXEYqbwbjk05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "def colorize_text(text):\n",
        "    for word, color in zip([\"Reasoning\", \"Question\", \"Answer\", \"Total time\"], [\"blue\", \"red\", \"green\", \"magenta\"]):\n",
        "        text = text.replace(f\"{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n",
        "    return text\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=query_pipeline)\n",
        "\n",
        "question = \"What is Chatbot and How it used in Finance Domain?\"\n",
        "response = llm(prompt=question)\n",
        "\n",
        "full_response =  f\"Question: {question}\\nAnswer: {response}\"\n",
        "display(Markdown(colorize_text(full_response)))"
      ],
      "metadata": {
        "id": "jqOxr-yqjkJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the RAG QA Chain using Langchain and Create Chatbot Interface"
      ],
      "metadata": {
        "id": "L8Gd9kUInJvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.llms import HuggingFaceHub\n",
        "from IPython.display import display, Markdown\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
        "\n",
        "# Define the prompt template\n",
        "template = \"\"\"\n",
        "You are a Finance QNA Expert, Analyze the Query and Respond to Customer with suitable answer. If you don't know the answer, just say \"Sorry, I don't know.\"\n",
        "Question: {question}\n",
        "Context: {context}\n",
        "Answer:\n",
        "\"\"\"\n",
        "PROMPT = PromptTemplate(input_variables=[\"context\", \"query\"], template=template)\n",
        "\n",
        "retriever = langchain_chroma.as_retriever(search_kwargs={\"k\": 1})\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm, retriever=retriever, chain_type_kwargs={\"prompt\": PROMPT}\n",
        ")\n",
        "\n",
        "def chat_with_rag():\n",
        "    print(\"Welcome to the GenAI Financial Chatbot. Type 'exit' to end the conversation.\")\n",
        "    while True:\n",
        "        query = input(\"You: \")\n",
        "        if query.lower() in [\"exit\", \"quit\"]:\n",
        "            break\n",
        "        context = \"Your context here\"  # Provide context if necessary, otherwise leave it empty\n",
        "        try:\n",
        "            result = qa_chain({\"context\": context, \"query\": query})\n",
        "            print(f\"Chatbot: {result['result']}\")\n",
        "        except RuntimeError as e:\n",
        "            print(f\"RuntimeError encountered: {e}\")\n",
        "\n",
        "# Run the chat\n",
        "chat_with_rag()"
      ],
      "metadata": {
        "id": "HHXu07hPnCCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use Another Dataset"
      ],
      "metadata": {
        "id": "rDbZK7oCnqrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "f = open(\"/kaggle/input/ecommerce-faq-chatbot-dataset/Ecommerce_FAQ_Chatbot_dataset.json\")\n",
        "data = json.load(f)"
      ],
      "metadata": {
        "id": "BWeZfBelnrn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = []\n",
        "answers = []\n",
        "\n",
        "for i in data[\"questions\"]:\n",
        "    questions += [i[\"question\"]]\n",
        "    answers += [i[\"answer\"]]"
      ],
      "metadata": {
        "id": "1YFqyoWtnuUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions[0], answers[0], data[\"questions\"][0]"
      ],
      "metadata": {
        "id": "56Ap-F_enwvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.DataFrame(questions, columns=['Questions'])\n",
        "data['Answers'] = answers\n",
        "data.head()"
      ],
      "metadata": {
        "id": "R-pIsh_Kn_Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"content\"] = data.apply(lambda row: f\"Question: {row['Questions']}\\nAnswer: {row['Answers']}\", axis=1)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "QtfpHAK1oDcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "id": "zPg1XdIwoECH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.docstore.document import Document\n",
        "\n",
        "# Prepare documents for LangChain\n",
        "documents = []\n",
        "for _, row in data.iterrows():\n",
        "    documents.append(Document(page_content=row[\"content\"]))"
      ],
      "metadata": {
        "id": "Qb2WmCqroG7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "id": "Sj06kUGAoHdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "hg_embeddings = HuggingFaceEmbeddings()"
      ],
      "metadata": {
        "id": "_mCMVifDoI6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "persist_directory = '/kaggle/working/'\n",
        "\n",
        "langchain_chroma = Chroma.from_documents(\n",
        "    documents=documents,\n",
        "    collection_name=\"chatbot_finance_ecom\",\n",
        "    embedding=hg_embeddings,\n",
        "    persist_directory=persist_directory\n",
        ")"
      ],
      "metadata": {
        "id": "KD3byUbjoLNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torch import cuda, bfloat16\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "from time import time\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "model_id = 'HuggingFaceH4/zephyr-7b-beta'\n",
        "\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "# set quantization configuration to load large model with less GPU memory\n",
        "# this requires the `bitsandbytes` library\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")\n",
        "\n",
        "print(device)\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "   model_id,\n",
        "    trust_remote_code=True,\n",
        "    max_new_tokens=1024\n",
        ")\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "LatZ27AioOLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the query pipeline with increased max_length\n",
        "query_pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.float16,\n",
        "    max_length=6000,  # Increase max_length\n",
        "    max_new_tokens=500,  # Control the number of new tokens generated\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "id": "6t_XULMDoQPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "def colorize_text(text):\n",
        "    for word, color in zip([\"Reasoning\", \"Question\", \"Answer\", \"Total time\"], [\"blue\", \"red\", \"green\", \"magenta\"]):\n",
        "        text = text.replace(f\"{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n",
        "    return text\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=query_pipeline)\n",
        "\n",
        "question = \"What is Chatbot and How it used in Finance Domain?\"\n",
        "response = llm(prompt=question)\n",
        "\n",
        "full_response =  f\"Question: {question}\\nAnswer: {response}\"\n",
        "display(Markdown(colorize_text(full_response)))"
      ],
      "metadata": {
        "id": "E2nHSd9UoxxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.llms import HuggingFaceHub\n",
        "from IPython.display import display, Markdown\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
        "\n",
        "# Define the prompt template\n",
        "template = \"\"\"\n",
        "You are a Finance QNA Expert, Analyze the Query and Respond to Customer with suitable answer. If you don't know the answer, just say \"Sorry, I don't know.\"\n",
        "Question: {question}\n",
        "Context: {context}\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(input_variables=[\"context\", \"query\"], template=template)\n",
        "\n",
        "retriever = langchain_chroma.as_retriever(search_kwargs={\"k\": 1})\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm, retriever=retriever, chain_type_kwargs={\"prompt\": PROMPT}\n",
        ")\n",
        "\n",
        "def chat_with_rag():\n",
        "    print(\"Welcome to the GenAI Financial Chatbot. Type 'exit' to end the conversation.\")\n",
        "    while True:\n",
        "        query = input(\"You: \")\n",
        "        if query.lower() in [\"exit\", \"quit\"]:\n",
        "            break\n",
        "        context = \"Your context here\"  # Provide context if necessary, otherwise leave it empty\n",
        "        try:\n",
        "            result = qa_chain({\"context\": context, \"query\": query})\n",
        "            print(f\"Chatbot: {result['result']}\")\n",
        "        except RuntimeError as e:\n",
        "            print(f\"RuntimeError encountered: {e}\")\n",
        "\n",
        "# Run the chat\n",
        "chat_with_rag()"
      ],
      "metadata": {
        "id": "Gxo1qpVMoyP_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}